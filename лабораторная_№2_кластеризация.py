# -*- coding: utf-8 -*-
"""Копия блокнота "Кластеризация"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZU-wC6XqVgfLZhAcvr46pZBarwxfVqYM

## Импорт библиотек
"""

!pip install yellowbrick

import numpy as np
import pandas as pd
import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import LabelEncoder
from yellowbrick.cluster import KElbowVisualizer
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, SnowballStemmer
import plotly.graph_objects as go
import plotly.express as px
from sklearn.manifold import TSNE

nltk.download('stopwords')

"""## Загрузка датасета"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Combined Data 1.csv', index_col=0)

df1 = df.copy()
df1.dropna(inplace = True)
#df1 = df1.drop('status', axis=1) # опускаем столбец с лейблами
df1.head()

"""## Препроцессинг"""

def clean_text(text):

    # Convert to string and lowercase
    text = str(text).lower()

    # Remove text in square brackets
    text = re.sub(r'\[.*?\]', '', text)

    # Remove URLs (including markdown-style links)
    text = re.sub(r'https?://\S+|www\.\S+|\[.*?\]\(.*?\)', '', text)

    # Remove HTML tags
    text = re.sub(r'<.*?>+', '', text)
    # Remove handles (that start with '@')
    text = re.sub(r'@\w+', '', text)

    # Remove punctuation and other special characters
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)

    # Remove newline characters
    text = re.sub(r'\n', ' ', text)

    # Remove words containing numbers
    text = re.sub(r'\w*\d\w*', '', text)

    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

stop_words = stopwords.words('english')
more_stopwords = ['u', 'im', 'c']
stop_words = stop_words + more_stopwords

stemmer = nltk.SnowballStemmer("english")

def preprocess_data(text):
    #Clean puntuation, urls, and so on
    text = clean_text(text)
    # Remove stopwords
    text = ' '.join(word for word in text.split(' ') if word not in stop_words)
    # Stemm all the words in the sentence
    #ext = ' '.join(stemmer.stem(word) for word in text.split(' '))

    return text

df2 = df1.copy()

df2['statement_clean'] = df2['statement'].apply(preprocess_data)

"""## Векторизация текстовых данных"""

documents = df2['statement_clean'].to_list()

vectorizer = TfidfVectorizer(norm='l2') # также заменялось на CountVectorizer
X = vectorizer.fit_transform(df2['statement_clean'])
X

pca = PCA(n_components=2)
PCA = pca.fit(X.toarray())
X_pca = pca.transform(X.toarray())

labels = df2['status']
encoder = LabelEncoder()
true_labels = encoder.fit_transform(labels)

"""# Поиск оптимального количества кластеров"""

kmeans = KMeans()
visualizer = KElbowVisualizer(kmeans, k=(1,10),size=(1080, 500))

visualizer.fit(X)
visualizer.show()

silhouette = []
K = range(2,10)

for k in K:
    kmeanModel = KMeans(n_clusters=k)
    preds = kmeanModel.fit_predict(X)
    silhouette.append(silhouette_score(X, preds))

plt.figure(figsize=(20,5))
plt.plot(K, silhouette, '-',color='g')
plt.xlabel('k values')
plt.ylabel('Silhouette score')
plt.title('Silhouette score of each k values')
plt.show()

num_clusters = 3
cluster_analyzer = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)
clusters = cluster_analyzer.fit_predict(X)

df2['cluster'] = clusters

cluster_centers = cluster_analyzer.cluster_centers_
labels = cluster_analyzer.labels_

"""# Визуализация"""

# Создаем DataFrame для визуализации
df_plot = pd.DataFrame({
    "x": X_pca[:, 0],
    "y": X_pca[:, 1],
    "cluster": labels,
    "text": df2['statement']
})

# Визуализация
fig = px.scatter(
    df_plot, x='x', y='y', color='cluster',
    hover_data=['text'],  # Показывать текст при наведении
    title="Interactive Clusters with Plotly",
    color_continuous_scale='viridis'
)

# Добавляем центроиды
fig.add_scatter(
    x=cluster_centers[:, 0],
    y=cluster_centers[:, 1],
    mode='markers',
    marker=dict(symbol='x', size=10, color='red'),
    name='Centroids'
)

fig.show()

"""# Средняя длина текстов"""

df2['text_length'] = df2['statement_clean'].apply(lambda x: len(x.split()))

# Calculate the average length
mean_lengths = df2.groupby('cluster')['text_length'].mean()
# Print them for simple result
print(mean_lengths)

"""# Метрики для оценки"""

silhouette = silhouette_score(X_pca, clusters)
ari = adjusted_rand_score(true_labels, clusters)

print(f"Silhouette Score: {silhouette:.2f}")
print(f"Adjusted Rand Index: {ari:.2f}")

"""# Топ частотных слов"""

def get_top_words(cluster_num, top_n=15):
    cluster_texts = df2[df2['cluster'] == cluster_num]['statement_clean']
    all_words = ' '.join(cluster_texts).split()
    word_counts = pd.Series(all_words).value_counts()
    return word_counts.head(top_n)

data = []
for i in range(num_clusters):
    top_words = get_top_words(i)
    data.append(go.Bar(name=f'Cluster {i}', x=top_words.index, y=top_words.values, text=top_words.values))

#data

fig = go.Figure(data=data)
fig.update_layout(
    barmode='group',
    xaxis_tickangle=-45,
    title='Top Words in Each Cluster',
    template='simple_white'
)
fig.show()

"""# Распределение тональности по кластерам"""

ct = pd.crosstab(index=true_labels, columns=labels, rownames=['Истинный класс'], colnames=['Кластер'])

print(ct)

"""# Fuzzy C-means"""

!pip install scikit-fuzzy

import skfuzzy as fuzz

n_clusters = 7
data = X.T

cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(
    data,
    n_clusters,
    m=1.5,
    error=0.005,
    maxiter=300,
    init=None
)

"""# Визуализация"""

max_membership = np.max(u, axis=0)
labels = np.argmax(u, axis=0)

# Визуализация
plt.figure(figsize=(6, 5))
scatter = plt.scatter(
    X_pca[:, 0],
    X_pca[:, 1],
    c=labels,
    cmap='tab10',
    alpha=max_membership,  # прозрачность — уровень уверенности
    s=60
)
plt.colorbar(scatter, label='Кластер')
plt.title("Нечеткая кластеризация")
plt.show()

"""# Распределение тональностей"""

print("\nCross-tabulation с основными кластерами:")
cross_tab = pd.crosstab(
    index=df2['cluster'],
    columns=df2['status'],
    margins=False
)
print(cross_tab)

"""# Нечеткая принадлежность"""

df2['Max_Membership'] = u.max(axis=0)
border_objects = df2[df2['Max_Membership'] < 0.7]  # Порог нечеткости

print(f"\nПограничные объекты ({len(border_objects)} шт.):")
print(border_objects[['status', 'cluster', 'Max_Membership']].sample(5, random_state=42))
